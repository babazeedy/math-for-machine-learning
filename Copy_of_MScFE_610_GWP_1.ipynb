{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXaBzpgG1mvR0KfEHL20eH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babazeedy/math-for-machine-learning/blob/main/Copy_of_MScFE_610_GWP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3a.**"
      ],
      "metadata": {
        "id": "_SGy4-nJP1s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use two distinct approaches:\n",
        "* **Backward Elimination** using the **Bayesian Information Criterion (BIC)**.<br>\n",
        "* **All Subsets Regression** using the **Adjusted $R^2$** and the **Akaike Information Criterion (AIC).**\n",
        "\n",
        "This is to balance goodness-of-fit with model complexity.\n",
        "\n",
        "1. Data Loading and Preparation"
      ],
      "metadata": {
        "id": "9mq4JPpYQdFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import itertools"
      ],
      "metadata": {
        "id": "-MVh3WLDQFJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"FE-GWP1_model_selection_2.csv\")"
      ],
      "metadata": {
        "id": "brU9ha_EQFaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['Y', 'Z1', 'Z2', 'Z3', 'Z4', 'Z5']"
      ],
      "metadata": {
        "id": "wukLRVnEevl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Y and X\n",
        "Y = df['Y']\n",
        "X = df[['Z1', 'Z2', 'Z3', 'Z4', 'Z5']]"
      ],
      "metadata": {
        "id": "8gLbUYAOb4wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add constant for the intercept term (required by statsmodels)\n",
        "X = sm.add_constant(X, prepend=False)"
      ],
      "metadata": {
        "id": "_vJgqJ-sb4zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Available predictors (excluding the constant)\n",
        "predictors = ['Z1', 'Z2', 'Z3', 'Z4', 'Z5']"
      ],
      "metadata": {
        "id": "v7vgaiCCb41q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset loaded. Sample size (N): {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXBy5Bn6b433",
        "outputId": "b7097c1e-bc97-4e41-a39e-f4a47836ccc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Sample size (N): 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **First Approach**<br>\n",
        "**Backward Elimination (Using BIC)**<br>\n",
        "**Justification**: Backward elimination systematically removes the least useful predictor at each step. By using the **Bayesian Information Criterion (BIC)**, we prioritize **parsimony** (simplicity). BIC heavily penalizes models with more parameters ($k$) than AIC, favoring the simplest model that adequately explains the data. We aim to **minimize BIC.**"
      ],
      "metadata": {
        "id": "h2tKFSg2hHpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_elimination_bic(X, Y, initial_vars):\n",
        "    \"\"\"Performs backward elimination using BIC as the criterion.\"\"\"\n",
        "\n",
        "    selected_vars = initial_vars + ['const']\n",
        "\n",
        "    # 1. Start with the full model\n",
        "    model_full = sm.OLS(Y, X[selected_vars]).fit()\n",
        "    best_bic = model_full.bic\n",
        "    best_model_vars = selected_vars.copy()\n",
        "\n",
        "    print(\"\\n--- First approach: Backward Elimination (Optimizing BIC) ---\")\n",
        "    print(f\"Starting BIC (Full Model: {len(initial_vars)} vars): {best_bic:.3f}\")\n",
        "\n",
        "    while len(selected_vars) > 1: # Stop when only intercept remains\n",
        "\n",
        "        candidates_to_remove = [var for var in selected_vars if var != 'const']\n",
        "        bic_scores = {}\n",
        "\n",
        "        # Check BIC for models formed by removing one variable\n",
        "        for var_to_remove in candidates_to_remove:\n",
        "            current_vars = [v for v in selected_vars if v != var_to_remove]\n",
        "            model = sm.OLS(Y, X[current_vars]).fit()\n",
        "            bic_scores[var_to_remove] = model.bic\n",
        "\n",
        "        # Find the variable whose removal results in the lowest BIC\n",
        "        best_removal = min(bic_scores, key=bic_scores.get)\n",
        "        new_bic = bic_scores[best_removal]\n",
        "\n",
        "        print(f\"\\nEvaluating removals from {len(selected_vars)-1} vars: {candidates_to_remove}\")\n",
        "\n",
        "        # Check stopping condition: only proceed if BIC improves (decreases)\n",
        "        if new_bic < best_bic:\n",
        "            print(f\"Removed {best_removal} -> New BIC: {new_bic:.3f} (IMPROVEMENT)\")\n",
        "            selected_vars.remove(best_removal)\n",
        "            best_bic = new_bic\n",
        "            best_model_vars = selected_vars.copy()\n",
        "        else:\n",
        "            print(f\"Removing {best_removal} -> New BIC: {new_bic:.3f} (WORSENING) -> STOP.\")\n",
        "            break\n",
        "\n",
        "    final_vars = [v for v in best_model_vars if v != 'const']\n",
        "    return final_vars, best_bic\n",
        "\n",
        "# Execute Backward Elimination\n",
        "bic_model_vars, best_bic_score = backward_elimination_bic(X, Y, predictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlBRCV4ib46I",
        "outputId": "3ed8b374-e6e0-4b0f-d636-e2d048e2ef07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First approach: Backward Elimination (Optimizing BIC) ---\n",
            "Starting BIC (Full Model: 5 vars): -150.271\n",
            "\n",
            "Evaluating removals from 5 vars: ['Z1', 'Z2', 'Z3', 'Z4', 'Z5']\n",
            "Removing Z1 -> New BIC: -105.160 (WORSENING) -> STOP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion from backward Elimination:**\n",
        "\n",
        "The process correctly stopped immediately at Step 1:\n",
        "* The BIC for the full model, $\\mathbf{-150.271}$, is the lowest BIC found.\n",
        "* Removing any single predictor causes the BIC to increase (i.e., become less negative), which signals a worsening of the model's overall fit/parsimony trade-off.\n",
        "\n",
        "Therefore, the best model found by the Backward Elimination process is the Full Model $\\{Z_1, Z_2, Z_3, Z_4, Z_5\\}$, with a BIC score of $\\mathbf{-150.271}$."
      ],
      "metadata": {
        "id": "JIEalIqW3iqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second approach:** <br>**All Subsets Regression (Using Adjusted $R^2$ and AIC)**<br>**Justification:** All Subsets Regression evaluates every possible combination of predictors, ensuring the true globally optimal subset for any given size is found. We will use two criteria for this approach:\n",
        "* **Adjusted $R^2$ ($\\bar{R}^2$)**: We aim to **maximize** $\\bar{R}^2$, as it indicates the highest proportion of variance explained after accounting for the number of variables.\n",
        "* **Akaike Information Criterion (AIC)**: We aim to **minimize AIC**, which provides a slightly less punitive penalty for complexity than BIC."
      ],
      "metadata": {
        "id": "3nCAkvu05Gd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def all_subsets_regression(X, Y, predictors):\n",
        "    \"\"\"Performs all subsets regression and selects the best models.\"\"\"\n",
        "\n",
        "    best_results = {'max_adj_r2': (-np.inf, []), 'min_aic': (np.inf, [])}\n",
        "\n",
        "    print(\"\\n---  Second approach: All Subsets Regression ---\")\n",
        "\n",
        "    # Iterate through all possible model sizes (k=1 to k=5)\n",
        "    for k in range(1, len(predictors) + 1):\n",
        "\n",
        "        # Generate all combinations of k predictors\n",
        "        for subset in itertools.combinations(predictors, k):\n",
        "\n",
        "            current_vars = list(subset) + ['const']\n",
        "            X_current = X[current_vars]\n",
        "\n",
        "            # Fit the model\n",
        "            model = sm.OLS(Y, X_current).fit()\n",
        "\n",
        "            # Calculate criteria\n",
        "            adj_r2 = model.rsquared_adj\n",
        "            aic = model.aic\n",
        "\n",
        "            # Check for best Adjusted R^2\n",
        "            if adj_r2 > best_results['max_adj_r2'][0]:\n",
        "                best_results['max_adj_r2'] = (adj_r2, subset)\n",
        "\n",
        "            # Check for best AIC\n",
        "            if aic < best_results['min_aic'][0]:\n",
        "                best_results['min_aic'] = (aic, subset)\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Total Models Evaluated: {2**len(predictors) - 1}\")\n",
        "    print(\"\\nModel Selection Summary:\")\n",
        "    print(f\"1. Best Model by MAX Adjusted R-Squared ({best_results['max_adj_r2'][0]:.4f}): {list(best_results['max_adj_r2'][1])}\")\n",
        "    print(f\"2. Best Model by MIN AIC ({best_results['min_aic'][0]:.3f}): {list(best_results['min_aic'][1])}\")\n",
        "\n",
        "    return best_results['max_adj_r2'][1], best_results['min_aic'][1]\n",
        "\n",
        "# Execute All Subsets Regression\n",
        "adj_r2_model_vars, aic_model_vars = all_subsets_regression(X, Y, predictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B94TBuSZb5By",
        "outputId": "e131793b-938e-4263-c600-c885b23d914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---  Second approach: All Subsets Regression ---\n",
            "Total Models Evaluated: 31\n",
            "\n",
            "Model Selection Summary:\n",
            "1. Best Model by MAX Adjusted R-Squared (0.9936): ['Z1', 'Z2', 'Z3', 'Z4', 'Z5']\n",
            "2. Best Model by MIN AIC (-165.902): ['Z1', 'Z2', 'Z3', 'Z4', 'Z5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_vars = ['Z1', 'Z2', 'Z3', 'Z4', 'Z5']\n",
        "X_final = X[final_vars + ['const']]\n",
        "final_model = sm.OLS(Y, X_final).fit()\n",
        "\n",
        "print(\"\\n--- Final Model (Z1, Z2, Z3, Z4, Z5) Summary ---\")\n",
        "print(final_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCkJAtKmb5Et",
        "outputId": "9a36b6b4-fb4c-4526-a0af-389ccaf6aba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Model (Z1, Z2, Z3, Z4, Z5) Summary ---\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.994\n",
            "Model:                            OLS   Adj. R-squared:                  0.994\n",
            "Method:                 Least Squares   F-statistic:                     3062.\n",
            "Date:                Mon, 15 Dec 2025   Prob (F-statistic):          2.07e-102\n",
            "Time:                        12:31:59   Log-Likelihood:                 88.951\n",
            "No. Observations:                 100   AIC:                            -165.9\n",
            "Df Residuals:                      94   BIC:                            -150.3\n",
            "Df Model:                           5                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Z1             0.4487      0.058      7.781      0.000       0.334       0.563\n",
            "Z2             0.2987      0.028     10.836      0.000       0.244       0.353\n",
            "Z3            -0.4065      0.010    -39.578      0.000      -0.427      -0.386\n",
            "Z4             1.0082      0.009    114.106      0.000       0.991       1.026\n",
            "Z5             0.2572      0.013     20.449      0.000       0.232       0.282\n",
            "const          1.0097      0.013     77.496      0.000       0.984       1.036\n",
            "==============================================================================\n",
            "Omnibus:                        0.377   Durbin-Watson:                   2.046\n",
            "Prob(Omnibus):                  0.828   Jarque-Bera (JB):                0.126\n",
            "Skew:                           0.072   Prob(JB):                        0.939\n",
            "Kurtosis:                       3.097   Cond. No.                         9.13\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion from the Results**<br>The OLS regression results show that the model:<br>\n",
        "* **Is Highly Significant and Excellent Fit:** The model is collectively highly significant (F-stat) and explains virtually all of the variation in $Y$ ($\\text{Adj.} R^2 = 0.994$).\n",
        "* Every Predictor is Relevant: Every single variable ($Z_1$ through $Z_5$) has a highly statistically significant unique contribution to predicting $Y$ ($p < 0.001$).\n",
        "* **Assumptions are Met:** Key assumptions regarding normality of residuals (Omnibus/JB tests) and autocorrelation (Durbin-Watson) are satisfied, and multicollinearity is not an issue.\n",
        "\n",
        "\n",
        "The detailed summary confirms that the **Full Model** ($\\{Z_1, Z_2, Z_3, Z_4, Z_5\\}$) is the best and most appropriate model for this dataset."
      ],
      "metadata": {
        "id": "jReve-UVEAAv"
      }
    }
  ]
}